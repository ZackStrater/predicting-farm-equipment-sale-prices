{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "olympic-plasma",
   "metadata": {},
   "source": [
    "# Linear models case study\n",
    "\n",
    "- [Linear models case study](#linear-models-case-study)\n",
    "  - [Topic Choices](#topic-choices)\n",
    "    - [Choice 1: Forecast HIV incidence in US counties](#choice-1-forecast-hiv-incidence-in-us-counties)\n",
    "      - [Major challenges](#major-challenges)\n",
    "    - [Choice 2: Predict the sale price of heavy equipment at auction](#choice-2-predict-the-sale-price-of-heavy-equipment-at-auction)\n",
    "      - [Major challenges](#major-challenges-1)\n",
    "  - [Deliverables](#deliverables)\n",
    "\n",
    "## Topic Choices\n",
    "\n",
    "Depending on your campus (check with your instructors), you and your group have \n",
    "two options for this case study.\n",
    "\n",
    "\n",
    "### Choice 1: [Forecast HIV incidence in US counties](forecast_HIV_infections/README.md)\n",
    "\n",
    "**Using data merged from several databases, you are asked to build a model that\n",
    "predicts HIV incidence for US counties.**  You should also identify and report\n",
    "the most significant drivers of HIV infection and how they vary between counties.\n",
    "\n",
    "Read more [here](./forecast_HIV_infections/README.md).\n",
    "\n",
    "#### Major challenges\n",
    "1. Non-normal distribution of features, possibly requires thoughtful, domain knowledge based feature engineering, data preprocessing or feature tranformation\n",
    "2. Require manual splitting of training data and test data.\n",
    "3. Data comes from various sources, which requires careful reading to understand the exact meaning of the data.\n",
    "4. Need to be aware of possible data leaking\n",
    "\n",
    "### Choice 2: [Predict the sale price of heavy equipment at auction](predict_auction_price/README.md)\n",
    "\n",
    "**Predict the sale price of a particular piece of heavy equipment at auction based\n",
    "on its usage, equipment type, and configuration.**  The data is sourced from auction\n",
    "result postings and includes information on usage and equipment configurations.\n",
    "\n",
    "Read more [here](./predict_auction_price/README.md).\n",
    "\n",
    "#### Major challenges\n",
    "1. Need to learn to use script to evaluate results\n",
    "2. Instruction information for completion is more than [Choice 1: Forecast HIV incidence in US counties](#choice-1-forecast-hiv-incidence-in-us-counties).\n",
    "3. Lots of missing data in data set.\n",
    "4. Need to be aware of possible data leaking\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "At the end of the day your group will be expected to present for 5-10\n",
    "minutes on your findings.  Present results from your README.md.\n",
    "\n",
    "Cover the following in your presentation.\n",
    "\n",
    "   1. Talk about what you planned to accomplish\n",
    "   2. How you organized yourselves as a team (including your git workflow)\n",
    "   3. Description of the problem and the data\n",
    "   4. What you accomplished (how you chose model, performance metric, validation)\n",
    "   5. Performance on unseen data\n",
    "   5. Anything new you learned along the way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-ratio",
   "metadata": {},
   "source": [
    "# Predict Heavy Equipment Auction Price\n",
    "\n",
    "- [Predict Heavy Equipment Auction Price](#predict-heavy-equipment-auction-price)\n",
    "  - [Case Study Goal](#case-study-goal)\n",
    "    - [Evaluation Metrics](#evaluation-metrics)\n",
    "  - [Backgrounds](#backgrounds)\n",
    "  - [Data](#data)\n",
    "  - [Overview of the `loss_model.py` script](#overview-of-the-loss_modelpy-script)\n",
    "  - [Credit](#credit)\n",
    "  - [Appendix](#appendix)\n",
    "    - [Important Tips](#important-tips)\n",
    "    - [Restrictions](#restrictions)\n",
    "\n",
    "## Case Study Goal\n",
    "Predict the `sale price` of a particular piece of heavy equipment at auction, based\n",
    "on its usage, equipment type, and configuration.  The data is sourced from auction\n",
    "result postings and includes information on usage and equipment configurations.\n",
    "\n",
    "### Evaluation Metrics\n",
    "The evaluation of your model will be based on Root Mean Squared Log Error.\n",
    "Which is computed as follows:\n",
    "\n",
    "![Root Mean Squared Logarithmic Error](images/rmsle.png)\n",
    "\n",
    "where *p<sub>i</sub>* are the predicted values (predicted auction sale prices) \n",
    "and *a<sub>i</sub>* are the actual values (the actual auction sale prices).\n",
    "\n",
    "Note that this loss function is sensitive to the *ratio* of predicted values to\n",
    "the actual values, a prediction of 200 for an actual value of 100 contributes\n",
    "approximately the same amount to the loss as a prediction of 2000 for an actual\n",
    "value of 1000.  To convince yourself of this, recall that a difference of\n",
    "logarithms is equal to a single logarithm of a ratio, and rewrite each summand\n",
    "as a single logarithm of a ratio.\n",
    "\n",
    "This loss function is implemented in [`loss_model.py`](./loss_model.py). Read it to understand how it works.\n",
    "\n",
    "## Backgrounds\n",
    "\n",
    "Please check the original Kaggle contest [page]((https://www.kaggle.com/c/bluebook-for-bulldozers)).\n",
    "\n",
    "## Data\n",
    "The data for this case study are in `./data`. Although there are both training\n",
    "and testing data sets, the testing data set [test.csv](./data/test.csv) should **`only`** be utilized to evaluate\n",
    "your final model performance at the end of the day. Also, the ground truth of the test data is in [ground_truth/test_actual.csv](./data/ground_truth/test_actual.csv). Think about it as your\n",
    "hold out set.  Use cross-validation on the training data set [train_1.csv](./data/train_1.csv) and [train_2.csv](./data/train_2.csv) to identify your\n",
    "best model and report the performance of your best model on the test data at the end of the day.\n",
    "\n",
    "> Hint: use `cat` to concatenate `train_1.csv` and `train_2.csv` to a single `train.csv` file.\n",
    "  ```bash\n",
    "  cat train_1.csv train_2.csv > train.csv\n",
    "  ```\n",
    "\n",
    "By using the same test data and the same evaluation metric (RMSLE) the relative\n",
    "performance of different group's models on this case study can be assessed.\n",
    "\n",
    "A [data_dictionary.csv](./data/data_dictionary.csv) is included that explains the columns in the data.\n",
    "\n",
    "## Overview of the `loss_model.py` script\n",
    "Included is a loss function to test your predictions of the test set against the provided hold out test set.  This follows a common setup in competitions such as Kaggle, where this came from.  In these types of setups, there is a labeled train set to do your modeling and feature tuning.  There is also a provided hold-out test set to compare your predictions against.  You will need to fit a model on the training data and get a prediction for all the data in the test set.  You will then need to create csv containing the field 'SalesID' and 'SalePrice' (must match exactly). An example file is created for you [Example_Output.csv](./data/Example_Output.csv). This csv file will be the input parameter to running the function.\n",
    "Example:\n",
    "In terminal:\n",
    "\n",
    "```bash\n",
    "python loss_model.py <path to csv file>\n",
    "```\n",
    "\n",
    "For example, the following command returns `0.7802091986822471`\n",
    "```bash\n",
    "python loss_model.py data/Example_Output.csv\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Credit\n",
    "This case study is based on [Kaggle's Blue Book for Bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers) competition.  The best RMSLE was only 0.23 (obviously lower is better).\n",
    ">  Note that if you were to simply guess the median auction price for all the pieces of equipment in the test set you would get an RMSLE of about 0.7.\n",
    "\n",
    "## Appendix\n",
    "\n",
    "### Important Tips\n",
    "\n",
    "1. This data is quite messy. Try to use your judgement about where your\n",
    "cleaning efforts will yield the most results and focus there first.\n",
    "2. Because of the restriction to linear models, you will have to carefully\n",
    "consider how to transform continuous predictors in your model.\n",
    "3. Remember any transformations you apply to the training data will also have\n",
    "to be applied to the testing data, so plan accordingly.\n",
    "4. Any transformations of the training data that *learn parameters* (for\n",
    "example, standardization learns the mean and variance of a feature) must only\n",
    "use parameters learned from the *training data*.\n",
    "5. It's possible some columns in the test data will take on values not seen in\n",
    "the training data. Plan accordingly.\n",
    "6. Use your intuition to *think about where the strongest signal about a price\n",
    "is likely to come from*. If you weren't fitting a model, but were asked to use\n",
    "this data to predict a price what would you do? Can you combine the model with\n",
    "your intuitive instincts?  This is important because it can be done *without\n",
    "looking at the data*; thinking about the problem has no risk of overfitting.\n",
    "7. Start simply. Fit a basic model and make sure you're able to get the submission \n",
    "working then iterate to improve.\n",
    "\n",
    "8. Remember that you are evaluated on a loss function that is only sensitive to\n",
    "the *ratios* of predicted to actual values.  It's almost certainly too much of\n",
    "a task to implement an algorithm that minimizes this loss function directly in\n",
    "the time you have, but there are some steps you can take to do a good job of\n",
    "it.\n",
    "\n",
    "### Restrictions\n",
    "Please use only *regression* methods for this case study.  The following techniques\n",
    "are allowed.\n",
    "\n",
    "  - Linear Regression.\n",
    "  - Logistic Regression.\n",
    "  - Median Regression (linear regression by minimizing the sum of absolute deviations).\n",
    "  - Any other [GLM](http://statsmodels.sourceforge.net/devel/glm.html).\n",
    "  - Regularization: Ridge and LASSO.\n",
    "\n",
    "You may use other models or algorithms as supplements (for example, in feature\n",
    "engineering), but your final submissions must be losses from a linear type\n",
    "model.\n",
    "\n",
    "## Credit\n",
    "This case study is based on [Kaggle's Blue Book for Bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers) competition.  The best RMSLE was only 0.23 (obviously lower is better).  Note\n",
    "that if you were to simply guess the median auction price for all the pieces of equipment in\n",
    "the test set you would get an RMSLE of about 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-velvet",
   "metadata": {},
   "source": [
    "# Forecasting-HIV-Infections Case Study\n",
    "- [Forecasting-HIV-Infections Case Study](#forecasting-hiv-infections-case-study)\n",
    "  - [Case Study Goal](#case-study-goal)\n",
    "  - [Background](#background)\n",
    "  - [Data](#data)\n",
    "    - [Data merging](#data-merging)\n",
    "  - [Credit](#credit)\n",
    "## Case Study Goal\n",
    "1)\tTo accurately model HIV `incidences` (new infections per 100,000) in US\n",
    "counties by building a linear regression model that utilizes HIV infection data, census data, data on the opioid crisis, and data on sexual orientation.\n",
    "\n",
    "2)\tIdentify features that are the most significant drivers of HIV infection rates and learn how these drivers differ between different regions.\n",
    "\n",
    "## Background\n",
    "Due to the development of anti-retroviral therapies the HIV/AIDS epidemic is \n",
    "generally considered to be under control in the US.  However, as of 2015 there \n",
    "were 971,524 people living with diagnosed HIV in the US with an estimation of \n",
    "37,600 new HIV diagnoses in 2014.  HIV infection rates continue to be particularly\n",
    "problematic in communities of color, among men who have sex with men (MSM), the\n",
    "transgender community, and other vulnerable populations in the US. Socioeconomic \n",
    "factors are a significant risk factor for HIV infection and likely contribute \n",
    "to HIV infection risk in these communities.  The current US opioid crisis has \n",
    "further complicated the efforts to combat HIV with HIV infection outbreaks now \n",
    "hitting regions that weren’t previously thought to be vulnerable to such outbreaks.  \n",
    "\n",
    "A model that can accurately forecast regional HIV infection rates would be \n",
    "beneficial to local public health officials.  Provided with this information, \n",
    "these officials will be able to better marshal the resources necessary to combat\n",
    "HIV and prevent outbreaks from occurring.  Accurate modeling will also identify \n",
    "risk factors for communities with high HIV infection rates and provide clues \n",
    "as to how officials may better combat HIV in their respective communities.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "\n",
    "The `./data` folder contains data from three publically available sources.  Groups should feel\n",
    "free to supplement this data if they wish.\n",
    "1. The largest collection of HIV and opioid data was obtained from the [opioid database](http://opioid.amfar.org/) maintained by the American Foundation for AIDS Research (amfAR).  \n",
    "2. Demographic and economic data were obtained from the 5yr - American Community Survey which are available at the [US census bureau website](https://factfinder.census.gov/faces/nav/jsf/pages/searchresults.xhtml?refresh=t).\n",
    "3. Estimates for the [MSM population](http://emorycamp.org/item.php?i=48) in each county were obtained from the Emory Coalition for Applied Modeling for Prevention (CAMP).\n",
    "\n",
    "Data dictionaries that indicate what each column in the data means are included in the folder associated with each data set.\n",
    "\n",
    "## Data merging  \n",
    "\n",
    "The `merge_data.ipynb` notebook reads and merges most of the data in the \n",
    "`data` folder into one dataframe.  Read through and execute this notebook cell-by-cell to\n",
    "better understand the data and bring it together for EDA.\n",
    "\n",
    "\n",
    "## Credit\n",
    "This case study is based on [Eric Logue's capstone project](https://github.com/elogue01/Forecasting-HIV-Infections).  \n",
    "You may wish to consult his Github repository devoted to this analysis for inspiration and insight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-rachel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-nancy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
